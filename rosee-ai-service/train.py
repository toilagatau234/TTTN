import os
import sys

if sys.stdout.encoding != 'utf-8':
    sys.stdout.reconfigure(encoding='utf-8')
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForTokenClassification,
    TrainingArguments,
    Trainer,
    DataCollatorForTokenClassification
)

# 1. KH·ªûI T·∫†O B·ªò NH√ÉN (LABEL VOCABULARY)
# T·∫≠p h·ª£p to√†n b·ªô c√°c nh√£n xu·∫•t hi·ªán trong 200 c√¢u data c·ªßa ch√∫ng ta
LABEL_LIST = [
    "O", 
    "B-FLOWER", "I-FLOWER", 
    "B-COLOR", "I-COLOR", 
    "B-OCCASION", "I-OCCASION", 
    "B-PRICE", "I-PRICE", 
    "B-STYLE", "I-STYLE"
]
# T·∫°o t·ª´ ƒëi·ªÉn mapping gi·ªØa Chu·ªói (String) v√† S·ªë (ID) ƒë·ªÉ AI hi·ªÉu
label2id = {label: i for i, label in enumerate(LABEL_LIST)}
id2label = {i: label for i, label in enumerate(LABEL_LIST)}

# 2. C·∫§U H√åNH KI·∫æN TR√öC M√î H√åNH
MODEL_CHECKPOINT = "vinai/phobert-base"
OUTPUT_DIR = "./model_weights/phobert-ner-flower"

def tokenize_and_align_labels(examples, tokenizer):
    """
    H√†m x·ª≠ l√Ω Subword Tokenization (BPE) th·ªß c√¥ng cho PhoBERT.
    Do PhoBERT kh√¥ng h·ªó tr·ª£ Fast Tokenizer, ch√∫ng ta ph·∫£i t·ª± cƒÉn ch·ªânh nh√£n.
    """
    tokenized_inputs = {
        "input_ids": [],
        "attention_mask": [],
        "labels": []
    }
    
    max_length = 64
    
    for i, (tokens, tags) in enumerate(zip(examples["tokens"], examples["ner_tags"])):
        input_ids = [tokenizer.cls_token_id] # <s>
        label_ids = [-100]
        
        for word, tag in zip(tokens, tags):
            # Tokenize t·ª´ng t·ª´
            word_tokens = tokenizer.encode(word, add_special_tokens=False)
            
            if len(word_tokens) > 0:
                # Token ƒë·∫ßu ti√™n c·ªßa t·ª´ nh·∫≠n nh√£n g·ªëc
                input_ids.append(word_tokens[0])
                label_ids.append(label2id[tag])
                
                # C√°c token sau c·ªßa c√πng m·ªôt t·ª´ nh·∫≠n nh√£n -100
                for sub_token in word_tokens[1:]:
                    input_ids.append(sub_token)
                    label_ids.append(-100)
        
        # Th√™m token k·∫øt th√∫c </s>
        input_ids.append(tokenizer.sep_token_id)
        label_ids.append(-100)
        
        # Truncation
        if len(input_ids) > max_length:
            input_ids = input_ids[:max_length]
            label_ids = label_ids[:max_length]
            
        # Attention mask
        attention_mask = [1] * len(input_ids)
        
        # Padding
        padding_length = max_length - len(input_ids)
        if padding_length > 0:
            input_ids += [tokenizer.pad_token_id] * padding_length
            label_ids += [-100] * padding_length
            attention_mask += [0] * padding_length
            
        tokenized_inputs["input_ids"].append(input_ids)
        tokenized_inputs["attention_mask"].append(attention_mask)
        tokenized_inputs["labels"].append(label_ids)

    return tokenized_inputs

def main():
    print("üöÄ ƒêang kh·ªüi ƒë·ªông ti·∫øn tr√¨nh Fine-Tuning PhoBERT...")

    # Load Tokenizer c·ªßa PhoBERT
    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)

    # Load Dataset t·ª´ file JSONL v·ª´a t·∫°o
    dataset = load_dataset("json", data_files={"train": "data/train_dataset.jsonl"})

    # Map dataset qua h√†m tokenize
    tokenized_datasets = dataset.map(
        lambda x: tokenize_and_align_labels(x, tokenizer),
        batched=True
    )

    # Load Pre-trained Model v√† g·∫Øn l·ªõp Classification l√™n ƒë·ªânh
    model = AutoModelForTokenClassification.from_pretrained(
        MODEL_CHECKPOINT,
        num_labels=len(LABEL_LIST),
        id2label=id2label,
        label2id=label2id
    )

    # C·∫•u h√¨nh si√™u tham s·ªë (Hyperparameters)
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        learning_rate=2e-5,          # T·ªëc ƒë·ªô h·ªçc t·ªëi ∆∞u cho Fine-tuning BERT
        per_device_train_batch_size=8, # Batch size nh·ªè cho m√°y c√° nh√¢n (RAM/VRAM)
        num_train_epochs=15,         # S·ªë v√≤ng l·∫∑p hu·∫•n luy·ªán (200 c√¢u th√¨ 15 v√≤ng l√† v·ª´a ƒë·ªß)
        weight_decay=0.01,           # Tr√°nh Overfitting
        save_strategy="epoch",       # L∆∞u tr·ªçng s·ªë sau m·ªói Epoch
        logging_steps=10,            # In log ra terminal
        overwrite_output_dir=True,
    )

    # B·ªô collator gi√∫p padding c√°c c√¢u trong c√πng 1 batch c√≥ ƒë·ªô d√†i b·∫±ng nhau
    data_collator = DataCollatorForTokenClassification(tokenizer)

    # Kh·ªüi t·∫°o Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        data_collator=data_collator,
        tokenizer=tokenizer,
    )

    # B·∫ÆT ƒê·∫¶U TRAINING
    print("üî• B·∫Øt ƒë·∫ßu Training. Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t ƒë·∫øn v√†i ch·ª•c ph√∫t t√πy c·∫•u h√¨nh m√°y...")
    trainer.train()

    # L∆∞u m√¥ h√¨nh ho√†n ch·ªânh sau khi train xong
    trainer.save_model(OUTPUT_DIR)
    print(f"‚úÖ Qu√° tr√¨nh hu·∫•n luy·ªán ho√†n t·∫•t! Tr·ªçng s·ªë ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {OUTPUT_DIR}")

if __name__ == "__main__":
    main()