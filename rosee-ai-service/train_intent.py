import os
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)

# 1. C·∫§U H√åNH M√î H√åNH V√Ä NH√ÉN
MODEL_CHECKPOINT = "vinai/phobert-base"
OUTPUT_DIR = "./model_weights/phobert-intent-classifier"

INTENT_LABELS = ["GREETING", "CREATE_BOUQUET", "ASK_PRICE_STOCK", "CHECK_POLICY", "OUT_OF_DOMAIN"]
label2id = {label: i for i, label in enumerate(INTENT_LABELS)}
id2label = {i: label for i, label in enumerate(INTENT_LABELS)}

def tokenize_function(examples, tokenizer):
    # Padding v√† Truncation cho ƒë·ªô d√†i t·ªëi ƒëa 64 token
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=64)

def main():
    print("üöÄ ƒêang kh·ªüi ƒë·ªông ti·∫øn tr√¨nh Fine-Tuning Chatbot Intent Classifier...")

    # Load Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)

    # Load Dataset
    dataset = load_dataset("json", data_files={"train": "data/train_intent.jsonl"})

    # Map qua h√†m tokenize
    tokenized_datasets = dataset.map(
        lambda x: tokenize_function(x, tokenizer),
        batched=True
    )

    # Load Pre-trained Model chuy√™n cho Sequence Classification
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_CHECKPOINT,
        num_labels=len(INTENT_LABELS),
        id2label=id2label,
        label2id=label2id
    )

    # C·∫•u h√¨nh si√™u tham s·ªë (Hyperparameters)
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        learning_rate=3e-5,
        per_device_train_batch_size=8, # Ph√¢n lo·∫°i c√¢u nh·∫π h∆°n NER, c√≥ th·ªÉ ƒë·∫©y batch size l√™n 16
        num_train_epochs=20,            # 12 Epochs l√† ƒë·ªß h·ªôi t·ª• cho classification
        weight_decay=0.01,
        save_strategy="epoch",
        logging_steps=5,
        overwrite_output_dir=True,
    )

    # Data collator t·ª± ƒë·ªông padding ƒë·ªông
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # Kh·ªüi t·∫°o Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        data_collator=data_collator,
        tokenizer=tokenizer,
    )

    print("üî• B·∫Øt ƒë·∫ßu Training...")
    trainer.train()

    # L∆∞u m√¥ h√¨nh
    trainer.save_model(OUTPUT_DIR)
    print(f"‚úÖ Qu√° tr√¨nh hu·∫•n luy·ªán ho√†n t·∫•t! Tr·ªçng s·ªë l∆∞u t·∫°i: {OUTPUT_DIR}")

if __name__ == "__main__":
    main()